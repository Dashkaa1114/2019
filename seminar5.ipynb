{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "3139Ba4ud3ad",
        "9dbCJjZNeLWu",
        "2gyx4UcDdv5n"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExcvckbHbKz",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1MbMZ_LfIZsNxLm5GOcQXaguMJ7CsSSX0\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Google Colab дээр нээх</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/dl-ub-summer-school/2019/blob/master/seminar5.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />GitHub дээр нээх</a>\n",
        "  </td>\n",
        "  <td>\n",
        "       <a target=\"_blank\" href=\"https://sites.google.com/view/dlub/dl-ub-2019\"><img src=\"https://avatars0.githubusercontent.com/u/52651086?s=32&v=4\">Зуны сургалтын вебсайт</a>\n",
        "   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Rd72q0Mogz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Transformer Implementation\n",
        "\n",
        "#### Кодны эх сурвалж:\n",
        "\n",
        "[Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799), Sainbayar Sukhbaatar et al. \n",
        "\n",
        "https://github.com/facebookresearch/adaptive-span\n",
        "    \n",
        "#### Анхаарах зүйлс:\n",
        "\n",
        "Эхлээд дараах үйлдлүүдийг дагаж хийгээд энэ нөтбүкийг өөрийн Драйвтаа хадгалж аваарай.\n",
        "\n",
        "1. Зүүн дээд буланд байгаа _File_ дээр дараад\n",
        "2. _Save copy in Drive_ дээр дарж өөрийн хувийг үүсгээд\n",
        "3. Үүсгэсэн Колаб нөтбүк дээрээ ажиллаарай.\n",
        "4. Дараа нь энэ Колаб нөтбүкээ Драйв доторх Colab Notebooks гэдэг нэртэй, автоматаар үүсдэг хавтсан дотроос олж үзээрэй.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYefQdaHqcmw",
        "colab_type": "text"
      },
      "source": [
        "#### Хэрэгтэй сан, функцүүдээ оруулж ирэх"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O-Oax5Mqffk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TijVMBXdG2E",
        "colab_type": "text"
      },
      "source": [
        "#### Өөрсдийн кэйст тохируулж өөрчилсөн gradient clipping функц, үүнийг ашигласан Adagrad класс"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY0qS3dTdDLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _clip_grad(clr, grad, group_grad_clip):\n",
        "    if group_grad_clip > 0:\n",
        "        norm = grad.norm(2).item()\n",
        "        if norm > group_grad_clip:\n",
        "            clr *= group_grad_clip / (norm + 1e-10)\n",
        "    return clr\n",
        "\n",
        "\n",
        "class AdagradWithGradClip(Adagrad):\n",
        "    \"\"\"Adagrad algoritm with custom gradient clipping\"\"\"\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 lr=1e-2,\n",
        "                 lr_decay=0,\n",
        "                 weight_decay=0,\n",
        "                 initial_accumulator_value=0,\n",
        "                 grad_clip=0):\n",
        "        Adagrad.__init__(self,\n",
        "                         params,\n",
        "                         lr=lr,\n",
        "                         lr_decay=lr_decay,\n",
        "                         weight_decay=weight_decay,\n",
        "                         initial_accumulator_value=initial_accumulator_value)\n",
        "        self.defaults['grad_clip'] = grad_clip\n",
        "        self.param_groups[0].setdefault('grad_clip', grad_clip)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad.data\n",
        "                state = self.state[p]\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    if p.grad.data.is_sparse:\n",
        "                        raise RuntimeError(\"weight_decay option is \"\n",
        "                                           \"not compatible with sparse \"\n",
        "                                           \"gradients\")\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                clr = (group['lr'] /\n",
        "                       (1 + (state['step'] - 1) * group['lr_decay']))\n",
        "\n",
        "                # clip\n",
        "                clr = _clip_grad(clr=clr,\n",
        "                                 grad=grad,\n",
        "                                 group_grad_clip=group['grad_clip'])\n",
        "\n",
        "                if grad.is_sparse:\n",
        "                    # the update is non-linear so indices must be unique\n",
        "                    grad = grad.coalesce()\n",
        "                    grad_indices = grad._indices()\n",
        "                    grad_values = grad._values()\n",
        "                    size = grad.size()\n",
        "\n",
        "                    def make_sparse(values):\n",
        "                        constructor = grad.new\n",
        "                        if grad_indices.dim() == 0 or values.dim() == 0:\n",
        "                            return constructor().resize_as_(grad)\n",
        "                        return constructor(grad_indices, values, size)\n",
        "                    state['sum'].add_(make_sparse(grad_values.pow(2)))\n",
        "                    std = state['sum']._sparse_mask(grad)\n",
        "                    std_values = std._values().sqrt_().add_(1e-10)\n",
        "                    p.data.add_(-clr, make_sparse(grad_values / std_values))\n",
        "                else:\n",
        "                    state['sum'].addcmul_(1, grad, grad)\n",
        "                    std = state['sum'].sqrt().add_(1e-10)\n",
        "                    p.data.addcdiv_(-clr, grad, std)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGvbDxisdj_7",
        "colab_type": "text"
      },
      "source": [
        "#### Орчин, дата, модел, оптимизац, сургалттай холбоотой параметрүүдийн default утга"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y1DrsukdpK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# arguments with their default values\n",
        "\n",
        "PARAMS_CONFIG = {\n",
        "    # env-specific\n",
        "    'env_params': {\n",
        "        'distributed': {\n",
        "            'action': 'store_true',\n",
        "            'default': False,\n",
        "            'help': 'enable distributed training.'\n",
        "                    '(otherwise will use all available GPUs with dataparallel)',\n",
        "            'dest': 'distributed'\n",
        "        },\n",
        "        'local_rank': {\n",
        "            'type': int,\n",
        "            'default': 0,\n",
        "            'help': 'used in distributed training',\n",
        "            'dest': 'local_rank'\n",
        "        },\n",
        "    },\n",
        "    # data-specific\n",
        "    'data_params': {\n",
        "        'data': {\n",
        "            'type': str,\n",
        "            'default': 'data/text8',\n",
        "            'help': 'data location '\n",
        "                    '(must contain train.txt, valid.txt and test.txt)',\n",
        "            'dest': 'data_path'\n",
        "        },\n",
        "    },\n",
        "    # model-specific\n",
        "    'model_params': {\n",
        "        'hid-sz': {\n",
        "            'type': int,\n",
        "            'default': 256,\n",
        "            'help': 'hidden size (i.e. model size)',\n",
        "            'dest': 'hidden_size'\n",
        "        },\n",
        "        'inner-hid-sz': {\n",
        "            'type': int,\n",
        "            'default': 1024,\n",
        "            'help': 'inner hidden size of FF layer',\n",
        "            'dest': 'inner_hidden_size'\n",
        "        },\n",
        "        'nlayers': {\n",
        "            'type': int,\n",
        "            'default': 8,\n",
        "            'help': 'number of layers',\n",
        "            'dest': 'nb_layers'\n",
        "        },\n",
        "        'block-sz': {\n",
        "            'type': int,\n",
        "            'default': 64,\n",
        "            'help': 'block size '\n",
        "                    '(the length of sequence to process in parallel)',\n",
        "            'dest': 'block_size'\n",
        "        },\n",
        "        'nheads': {\n",
        "            'type': int,\n",
        "            'default': 2,\n",
        "            'help': 'number of self-attention heads',\n",
        "            'dest': 'nb_heads'\n",
        "        },\n",
        "        'attn-span': {\n",
        "            'type': int,\n",
        "            'default': 32,\n",
        "            'help': 'length of the attention span',\n",
        "            'dest': 'attn_span'\n",
        "        },\n",
        "        'dropout': {\n",
        "            'type': float,\n",
        "            'default': 0.2,\n",
        "            'help': 'dropout rate of ReLU and attention',\n",
        "            'dest': 'dropout'\n",
        "        },\n",
        "    },\n",
        "    # optimization-specific\n",
        "    'optim_params': {\n",
        "        'lr': {\n",
        "            'type': float,\n",
        "            'default': 0.03,\n",
        "            'help': 'learning rate',\n",
        "            'dest': 'lr'\n",
        "        },\n",
        "        'momentum': {\n",
        "            'type': float,\n",
        "            'default': 0.9,\n",
        "            'help': 'SGD momentum',\n",
        "            'dest': 'momentum'\n",
        "        },\n",
        "        'optim': {\n",
        "            'type': str,\n",
        "            'default': 'sgd',\n",
        "            'help': 'optimization method: sgd | adagrad',\n",
        "            'dest': 'optim'\n",
        "        },\n",
        "        'lr-warmup': {\n",
        "            'type': int,\n",
        "            'default': 0,\n",
        "            'help': 'linearly increase LR from 0 '\n",
        "                    'during first lr_warmup updates',\n",
        "            'dest': 'lr_warmup'\n",
        "        },\n",
        "        'grad-clip': {\n",
        "            'type': float,\n",
        "            'default': 0,\n",
        "            'help': '[only works with adagrad!] '\n",
        "                    'clip gradient of each module parameters by a given '\n",
        "                    'value',\n",
        "            'dest': 'grad_clip'\n",
        "        },\n",
        "    },\n",
        "    # trainer-specific\n",
        "    'trainer_params': {\n",
        "        'batch-sz': {\n",
        "            'type': int,\n",
        "            'default': 64,\n",
        "            'help': 'batch size',\n",
        "            'dest': 'batch_size'\n",
        "        },\n",
        "        'batch-split': {\n",
        "            'type': int,\n",
        "            'default': 1,\n",
        "            'help': 'split a batch into smaller parts to fit in GPU memory',\n",
        "            'dest': 'batch_split'\n",
        "        },\n",
        "        'nbatches': {\n",
        "            'type': int,\n",
        "            'default': 1000,\n",
        "            'help': 'number of batches in each iteration',\n",
        "            'dest': 'nb_batches_per_iter'\n",
        "        },\n",
        "        'niter': {\n",
        "            'type': int,\n",
        "            'default': 1000,\n",
        "            'help': 'number of iterations to train',\n",
        "            'dest': 'nb_iter'\n",
        "        },\n",
        "        'checkpoint': {\n",
        "            'type': str,\n",
        "            'default': '',\n",
        "            'help': 'path to save/load model',\n",
        "            'dest': 'checkpoint_path'\n",
        "        },\n",
        "        'full-eval-mode': {\n",
        "            'action': 'store_true',\n",
        "            'default': False,\n",
        "            'help': 'do evaluation on the whole validation and the test data',\n",
        "            'dest': 'full_eval_mode'\n",
        "        },\n",
        "    },\n",
        "    # adaptive attention span specific params\n",
        "    'adapt_span_params': {\n",
        "        'adapt-span': {\n",
        "            'action': 'store_true',\n",
        "            'default': False,\n",
        "            'help': 'enable adaptive attention span',\n",
        "            'dest': 'adapt_span_enabled'\n",
        "        },\n",
        "        'adapt-span-loss': {\n",
        "            'type': float,\n",
        "            'default': 0,\n",
        "            'help': 'the loss coefficient for span lengths',\n",
        "            'dest': 'adapt_span_loss'\n",
        "        },\n",
        "        'adapt-span-ramp': {\n",
        "            'type': int,\n",
        "            'default': 32,\n",
        "            'help': 'ramp length of the soft masking function',\n",
        "            'dest': 'adapt_span_ramp'\n",
        "        },\n",
        "        'adapt-span-init': {\n",
        "            'type': float,\n",
        "            'default': 0,\n",
        "            'help': 'initial attention span ratio',\n",
        "            'dest': 'adapt_span_init'\n",
        "        },\n",
        "        'adapt-span-cache': {\n",
        "            'action': 'store_true',\n",
        "            'default': False,\n",
        "            'help': 'adapt cache size as well to reduce memory usage',\n",
        "            'dest': 'adapt_span_cache'\n",
        "        },\n",
        "    },\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFXfgZGDdp53",
        "colab_type": "text"
      },
      "source": [
        "#### Дата токенчлох, корпус үүсгэх, дата хуваах функцүүд"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJClSZDuduHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _tokenize(text_path, dictionary_to_update):\n",
        "    \"\"\"Tokenizes a text file.\"\"\"\n",
        "    print('Tokenizing {}'.format(text_path))\n",
        "    assert os.path.exists(text_path)\n",
        "\n",
        "    nb_tokens_in_dictionary = len(dictionary_to_update)\n",
        "\n",
        "    # Count nb of tokens in text and update the dictionary\n",
        "    with open(text_path, 'r', encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            tokens = line.split() + ['<eos>']\n",
        "            for token in tokens:\n",
        "                if token not in dictionary_to_update:\n",
        "                    dictionary_to_update[token] = nb_tokens_in_dictionary\n",
        "                    nb_tokens_in_dictionary += 1\n",
        "\n",
        "    # Assign to each token its identifier\n",
        "    ids = []\n",
        "    with open(text_path, 'r', encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            tokens = line.split() + ['<eos>']\n",
        "            for token in tokens:\n",
        "                ids.append(dictionary_to_update[token])\n",
        "    ids = torch.LongTensor(ids)\n",
        "\n",
        "    return ids\n",
        "\n",
        "\n",
        "class Corpus:\n",
        "    def __init__(self, data_path):\n",
        "        self._dictionary = {}\n",
        "        self.train = _tokenize(\n",
        "            text_path=os.path.join(data_path, 'train.txt'),\n",
        "            dictionary_to_update=self._dictionary)\n",
        "        self.valid = _tokenize(\n",
        "            text_path=os.path.join(data_path, 'valid.txt'),\n",
        "            dictionary_to_update=self._dictionary)\n",
        "        self.test = _tokenize(\n",
        "            text_path=os.path.join(data_path, 'test.txt'),\n",
        "            dictionary_to_update=self._dictionary)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self._dictionary)\n",
        "\n",
        "\n",
        "def _batchify(data_tensor, batch_size):\n",
        "    nb_batches = data_tensor.size(0) // batch_size\n",
        "    # trim away some tokens to make whole batches\n",
        "    data_tensor = data_tensor.narrow(0, 0, nb_batches * batch_size)\n",
        "    data_tensor = data_tensor.view(batch_size, -1).contiguous()\n",
        "    return data_tensor\n",
        "\n",
        "\n",
        "def _build_corpus(data_path, env_params):\n",
        "    # save the corpus to a file so that it's faster next time\n",
        "    corpus_path = os.path.join(data_path, 'corpus.pt')\n",
        "    if os.path.exists(corpus_path):\n",
        "        print('Loading an existing corpus file from {}'.format(corpus_path))\n",
        "        corpus = torch.load(corpus_path)\n",
        "    else:\n",
        "        print('Creating a corpus file at {}'.format(corpus_path))\n",
        "        if env_params['distributed']:\n",
        "            # only one process need to create a corpus file\n",
        "            if env_params['rank'] == 0:\n",
        "                corpus = Corpus(data_path)\n",
        "                torch.save(corpus, corpus_path)\n",
        "                # sync with other processes\n",
        "                torch.distributed.broadcast(torch.zeros(1).cuda(), src=0)\n",
        "            else:\n",
        "                print('Waiting rank0 to create a corpus file.')\n",
        "                # sync with rank0\n",
        "                torch.distributed.broadcast(torch.zeros(1).cuda(), src=0)\n",
        "                corpus = torch.load(corpus_path)\n",
        "        else:\n",
        "            corpus = Corpus(data_path)\n",
        "            torch.save(corpus, corpus_path)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def _get_train_val_test_data(corpus, batch_size):\n",
        "    return [\n",
        "        _batchify(corpus.train, batch_size),\n",
        "        _batchify(corpus.valid, batch_size),\n",
        "        _batchify(corpus.test, batch_size)\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_train_val_test_data(data_params, env_params, batch_size, device):\n",
        "    corpus = _build_corpus(**data_params, env_params=env_params)\n",
        "    data_params['vocab_size'] = corpus.vocab_size\n",
        "    train_data, val_data, test_data = _get_train_val_test_data(\n",
        "        corpus=corpus, batch_size=batch_size)\n",
        "\n",
        "    if env_params['distributed']:\n",
        "        # split the data into equal parts\n",
        "        assert batch_size % env_params['world_size'] == 0\n",
        "        device_batch_size = batch_size // env_params['world_size']\n",
        "        slice_data = slice(\n",
        "            device_batch_size * env_params['rank'],\n",
        "            device_batch_size * (env_params['rank'] + 1))\n",
        "        train_data = train_data[slice_data]\n",
        "        val_data = val_data[slice_data]\n",
        "        test_data = test_data[slice_data]\n",
        "\n",
        "    train_data = train_data.to(device)\n",
        "    val_data = val_data.to(device)\n",
        "    test_data = test_data.to(device)\n",
        "    return train_data, val_data, test_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjNjxek2dyRj",
        "colab_type": "text"
      },
      "source": [
        "#### [TODO 1 - TODO 7]  Attention, Multi-head Attention, Feedforward, Transformer давхаргуудын классууд, Transformer-ын класс"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjXg1ull3rp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Size notations:\n",
        "# B = batch_size, H = hidden_size, M = block_size, L = attn_span\n",
        "\n",
        "\n",
        "def _skew(X, pad_value):\n",
        "    \"\"\"shift every row 1 step to right\"\"\"\n",
        "    # X = B x M x L\n",
        "    B, M, L = X.size()\n",
        "    X = F.pad(X, (0, M + 1), value=pad_value)  # B x M x (L+M+1)\n",
        "    X = X.view(B, -1)  # B x ML+MM+M\n",
        "    X = X[:, :-M]  # B x ML+MM\n",
        "    X = X.view(B, M, M + L)  # B x M x L+M\n",
        "    return X\n",
        "\n",
        "\n",
        "def _unskew(X):\n",
        "    \"\"\"reverse _skew operation\"\"\"\n",
        "    # X = B x M x L+M\n",
        "    B, M, L = X.size()\n",
        "    L -= M\n",
        "    X = X.view(B, -1)  # B x ML+MM\n",
        "    X = F.pad(X, (0, M))  # B x ML+MM+M\n",
        "    X = X.view(B, M, M + L + 1)  # B x M x L+M+1\n",
        "    X = X[:, :, :L]  # B x M x L\n",
        "    return X\n",
        "\n",
        "\n",
        "class SeqAttention(nn.Module):\n",
        "    \"\"\"Sequential self-attention layer.\n",
        "    Each token will attend to its previous fixed number of steps.\n",
        "    Note that attention doesn't include the current step itself.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, attn_span,\n",
        "                 dropout, adapt_span_params, **kargs):\n",
        "        nn.Module.__init__(self)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden_size = hidden_size # size of a single head\n",
        "        self.attn_span = attn_span\n",
        "        self.adapt_span_enabled = False\n",
        "\n",
        "    def forward(self, query, key, value, key_pe):\n",
        "        # query size = B x M x H\n",
        "        # key, value sizes = B x (M+L) x H\n",
        "\n",
        "        # compute attention from context\n",
        "        # B x M (dest) x (M+L) (src)\n",
        "        attn_cont = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attn_cont = _unskew(attn_cont)  # B x M x L\n",
        "\n",
        "        # compute the effect of position embedding\n",
        "        attn_pos = torch.matmul(query, key_pe)  # B x M x L_pos\n",
        "        \n",
        "        #TODO 1\n",
        "        #Add positional encoding value to attention\n",
        "        attn = attn_cont + attn_pos\n",
        "        \n",
        "        # TODO 2 \n",
        "        # Scale attention scores with square root of d_k. HINT: Result dimention # B x M X L_pos\n",
        "        attn = attn / math.sqrt(self.hidden_size)\n",
        "        \n",
        "        # TODO 3 \n",
        "        # Compute softmax on attentions scores\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        \n",
        "        # TODO 4 \n",
        "        # For regularization purpose, use dropout on attention scores. HINT: result dimention B x M X L_pos\n",
        "        attn = self.dropout(attn)\n",
        "        attn_cont = _skew(attn, 0)  # B x M X (L+M)\n",
        "        \n",
        "        # TODO 5\n",
        "        # Compute attention output using attention scores and value vectors. HINT: result dimention B x M x H\n",
        "        out = torch.matmul(attn_cont, value)\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def get_cache_size(self):\n",
        "        return self.attn_span\n",
        "\n",
        "\n",
        "class MultiHeadSeqAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, nb_heads, **kargs):\n",
        "        nn.Module.__init__(self)\n",
        "        assert hidden_size % nb_heads == 0\n",
        "        self.nb_heads = nb_heads\n",
        "        self.head_dim = hidden_size // nb_heads\n",
        "        self.attn = SeqAttention(\n",
        "            hidden_size=self.head_dim, nb_heads=nb_heads, **kargs)\n",
        "        self.proj_query = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.proj_out = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.proj_val = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.proj_key = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def head_reshape(self, x):\n",
        "        K = self.nb_heads\n",
        "        D = self.head_dim\n",
        "        x = x.view(x.size()[:-1] + (K, D))  # B x (M+L) x K x D\n",
        "        x = x.transpose(1, 2).contiguous()  # B x K x (M+L) x D\n",
        "        x = x.view(-1, x.size(-2), x.size(-1))  # B_K x (M+L) x D\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, key_pe):\n",
        "        B = query.size(0)\n",
        "        K = self.nb_heads\n",
        "        D = self.head_dim\n",
        "        M = query.size(1)\n",
        "\n",
        "        query = self.proj_query(query)\n",
        "        query = self.head_reshape(query)\n",
        "        value = self.proj_val(value)\n",
        "        value = self.head_reshape(value)\n",
        "        key = self.proj_key(key)\n",
        "        key = self.head_reshape(key)\n",
        "\n",
        "        out = self.attn(query, key, value, key_pe)  # B_K x M x D\n",
        "        out = out.view(B, K, M, D)  # B x K x M x D\n",
        "        out = out.transpose(1, 2).contiguous()  # B x M x K x D\n",
        "        out = out.view(B, M, -1)  # B x M x K_D\n",
        "        out = self.proj_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, inner_hidden_size, dropout, **kargs):\n",
        "        nn.Module.__init__(self)\n",
        "        self.fc1 = nn.Linear(hidden_size, inner_hidden_size)\n",
        "        self.fc2 = nn.Linear(inner_hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, h):\n",
        "        # TODO 6 \n",
        "        # Using self.fc1,  linearly tranform attention output, h. \n",
        "        # Use activation function RELU\n",
        "        h1 = F.relu(self.fc1(h))\n",
        "        \n",
        "        # TODO 7\n",
        "        # For regularization purpose, add dropout to h1 above.\n",
        "        h1 = self.dropout(h1)\n",
        "        \n",
        "        # TODO 8 \n",
        "        # Using self.fc2 to linearly tranform h1. \n",
        "        h2 = self.fc2(h1)\n",
        "        \n",
        "        return h2\n",
        "\n",
        "\n",
        "class TransformerSeqLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, **kargs):\n",
        "        nn.Module.__init__(self)\n",
        "        self.attn = MultiHeadSeqAttention(hidden_size=hidden_size, **kargs)\n",
        "        #self.attn = SeqAttention(hidden_size=hidden_size, **kargs)\n",
        "        self.ff = FeedForwardLayer(hidden_size=hidden_size, **kargs)\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, h, h_cache, key_pe):\n",
        "        # h = B x M x H\n",
        "        # h_cache = B x L x H\n",
        "        \n",
        "        #TODO 9\n",
        "        #Concatenate curent blocks with previous cache. HINT: output dimension B x (M+L) x H\n",
        "        h_all = torch.cat([h_cache, h], dim=1)\n",
        "        \n",
        "        attn_out = self.attn(h, h_all, h_all, key_pe)\n",
        "        \n",
        "        #TODO 10\n",
        "        #Normalize attention output. HINT:  output dimension # B x M x H\n",
        "        h = self.norm1(h + attn_out)\n",
        "        \n",
        "        #TODO 11\n",
        "        #Get the output from feedforward  NN \n",
        "        ff_out = self.ff(h)\n",
        "        \n",
        "        #TODO 12\n",
        "        #Normalize the output. HINT: output dimension  B x M x H\n",
        "        out = self.norm2(h + ff_out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerSeq(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, nb_heads, nb_layers,\n",
        "                 attn_span, **kargs):\n",
        "        nn.Module.__init__(self)\n",
        "        # token embeddings\n",
        "        self.in_emb = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.out_emb = nn.Linear(hidden_size, vocab_size)\n",
        "        # position embeddings\n",
        "        self.key_pe = nn.Parameter(\n",
        "            torch.randn(1, hidden_size // nb_heads, attn_span))\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.extend(\n",
        "            TransformerSeqLayer(\n",
        "                hidden_size=hidden_size, nb_heads=nb_heads,\n",
        "                attn_span=attn_span, **kargs)\n",
        "            for _ in range(nb_layers))\n",
        "\n",
        "    def forward(self, x, h_cache):\n",
        "        # x size = B x M\n",
        "        block_size = x.size(1)\n",
        "        h = self.in_emb(x)  # B x M x H\n",
        "        h_cache_next = []\n",
        "        for l, layer in enumerate(self.layers):\n",
        "            cache_size = layer.attn.attn.get_cache_size()\n",
        "            if cache_size > block_size:\n",
        "                h_cache_next_l = torch.cat(\n",
        "                    [h_cache[l][:, -cache_size + block_size:, :], h],\n",
        "                    dim=1).detach()\n",
        "            else:\n",
        "                h_cache_next_l = h[:, -cache_size:, :].detach()\n",
        "            h_cache_next.append(h_cache_next_l)\n",
        "            h = layer(h, h_cache[l], self.key_pe)  # B x M x H\n",
        "\n",
        "        out = F.log_softmax(self.out_emb(h), dim=-1)\n",
        "\n",
        "        return out, h_cache_next\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3139Ba4ud3ad",
        "colab_type": "text"
      },
      "source": [
        "#### Сургалтад хэрэглэгдэх, сургасан моделийг үнэлэх функцүүд"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuI-ckh1eJdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _train_step(model, X, Y, h_cache, eval_only, loss_div=1):\n",
        "    \"\"\"Single training step.\"\"\"\n",
        "\n",
        "    out, h_cache = model(X, h_cache)\n",
        "    out = out.view(-1, out.size(-1))\n",
        "    loss = torch.nn.functional.nll_loss(out, Y.view(-1))\n",
        "    loss_value = loss.item() / loss_div\n",
        "\n",
        "    if not eval_only:\n",
        "        # loss term from adaptive-span\n",
        "        if model.module.layers[0].attn.attn.adapt_span_enabled:\n",
        "            loss += sum(layer.attn.attn.adaptive_span.get_loss()\n",
        "                        for layer in model.module.layers)\n",
        "\n",
        "        (loss / loss_div).backward()\n",
        "\n",
        "    return loss_value, h_cache\n",
        "\n",
        "\n",
        "def _train_batch(model, optimizer, scheduler, X, Y, h_cache,\n",
        "                 eval_only, batch_split):\n",
        "    \"\"\"Train on a batch.\"\"\"\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch_split == 1:\n",
        "        # process a batch in a single step (default behaviour)\n",
        "        loss_value, h_cache = _train_step(model, X, Y, h_cache, eval_only)\n",
        "    else:\n",
        "        # split a batch into multiple pieces that each can fit in memory\n",
        "        assert X.size(0) % batch_split == 0\n",
        "        split_size = X.size(0) // batch_split\n",
        "        loss_value = 0\n",
        "        h_cache_list = []\n",
        "        for split_ind in range(batch_split):\n",
        "            split_slice = slice(split_ind*split_size, (split_ind+1)*split_size)\n",
        "            split_h_cache = [h[split_slice,:,:] for h in h_cache]\n",
        "            split_loss_value, split_h_cache = _train_step(\n",
        "                model, X[split_slice,:], Y[split_slice],\n",
        "                split_h_cache, eval_only, batch_split)\n",
        "            loss_value += split_loss_value\n",
        "            h_cache_list.append(split_h_cache)\n",
        "        h_cache = [\n",
        "            torch.cat(\n",
        "                [h_cache_list[i][l] for i in range(batch_split)]\n",
        "            , dim=0) for l in range(len(h_cache))]\n",
        "\n",
        "    if not eval_only:\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        optimizer.step()\n",
        "\n",
        "        # make sure span parameters are in a correct range\n",
        "        if model.module.layers[0].attn.attn.adapt_span_enabled:\n",
        "            for layer in model.module.layers:\n",
        "                layer.attn.attn.adaptive_span.clamp_param()\n",
        "\n",
        "    return loss_value, h_cache\n",
        "\n",
        "\n",
        "def train_iteration(model, optimizer, scheduler, data, nb_batches_per_iter,\n",
        "                    block_size, eval_only, train_pos, h_cache, batch_split):\n",
        "    \"\"\"Single training iteration.\"\"\"\n",
        "    if eval_only:\n",
        "        model.eval()\n",
        "    else:\n",
        "        model.train()\n",
        "\n",
        "    nb_batches_per_iter_max = nb_batches_per_iter\n",
        "    if eval_only:\n",
        "        # eval on fewer batches during training for speed-up\n",
        "        nb_batches_per_iter_max = max(1, nb_batches_per_iter // 10)\n",
        "        nb_batches_per_iter_max = min(nb_batches_per_iter_max,\n",
        "                                      math.ceil(data.size(1) / block_size))\n",
        "\n",
        "    loss_all = 0\n",
        "    actual_nb_batches_per_iter = 0\n",
        "    for _ in range(nb_batches_per_iter_max):\n",
        "        actual_nb_batches_per_iter += 1\n",
        "        X = data[:, train_pos: train_pos + block_size].contiguous()\n",
        "        Y = data[:, train_pos + 1: train_pos + block_size + 1].contiguous()\n",
        "\n",
        "        loss, h_cache = _train_batch(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            X=X, Y=Y,\n",
        "            h_cache=h_cache,\n",
        "            eval_only=eval_only,\n",
        "            batch_split=batch_split)\n",
        "        loss_all += loss\n",
        "        train_pos += block_size\n",
        "        if train_pos >= data.size(1) - block_size:\n",
        "            # reached the end. randomize the offset to reduce overfitting\n",
        "            train_pos = random.randrange(block_size)\n",
        "            # reset the cache\n",
        "            for h in h_cache:\n",
        "                h.fill_(0)\n",
        "\n",
        "    loss_all = loss_all / actual_nb_batches_per_iter\n",
        "    return loss_all, train_pos, h_cache\n",
        "\n",
        "\n",
        "# do full evaluation\n",
        "def full_eval(model, optimizer, scheduler, data, block_size, hidden_size):\n",
        "    model.eval()\n",
        "    train_pos = 0\n",
        "    nb_batches_per_iter_max = math.ceil(data.size(1) / block_size)\n",
        "    h_cache = [\n",
        "        torch.zeros(\n",
        "            data.size(0),\n",
        "            layer.attn.attn.get_cache_size(),\n",
        "            hidden_size).to(data.device)\n",
        "        for layer in model.module.layers]\n",
        "\n",
        "    loss_all = 0\n",
        "    actual_nb_batches_per_iter = 0\n",
        "    for _ in range(nb_batches_per_iter_max):\n",
        "        actual_nb_batches_per_iter += 1\n",
        "        X = data[:, train_pos: train_pos + block_size].contiguous()\n",
        "        Y = data[:, train_pos + 1: train_pos + block_size + 1].contiguous()\n",
        "\n",
        "        loss, h_cache = _train_batch(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            X=X, Y=Y,\n",
        "            h_cache=h_cache,\n",
        "            eval_only=True,\n",
        "            batch_split=1)\n",
        "        loss_all += loss\n",
        "        train_pos += block_size\n",
        "        if train_pos >= data.size(1) - block_size:\n",
        "            # Skip the remaining tokens as it can't make a whole block.\n",
        "            # An effect on performance should be negligable for a large data.\n",
        "            break\n",
        "\n",
        "    loss_all = loss_all / actual_nb_batches_per_iter\n",
        "    return loss_all\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dbCJjZNeLWu",
        "colab_type": "text"
      },
      "source": [
        "#### Параметр унших, орчин тохируулах, оптимизацчиллын, сурах явцдаа модел хадгалах, унших, явц хэвлэх зэрэг хэрэглээний функцүүд"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFO-Zqv1eNqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_params(args, params_config):\n",
        "    params = {}\n",
        "    for params_category in params_config:\n",
        "        params[params_category] = {}\n",
        "        for param_config in params_config[params_category]:\n",
        "            try:\n",
        "                params[params_category][params_config[params_category][param_config]['dest']] = args[param_config]\n",
        "            except KeyError:\n",
        "                params[params_category][params_config[params_category][param_config]['dest']] = params_config[params_category][param_config]['default']\n",
        "    return (params[\"env_params\"], params[\"model_params\"], params[\"adapt_span_params\"], params[\"optim_params\"], params[\"data_params\"], params[\"trainer_params\"])\n",
        "\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# ENVIRONMENT\n",
        "##############################################################################\n",
        "\n",
        "def _torch_distributed_init_process_group(local_rank):\n",
        "    torch.distributed.init_process_group(\n",
        "        backend='nccl',\n",
        "        init_method='env://'\n",
        "    )\n",
        "    rank = torch.distributed.get_rank()\n",
        "    world_size = torch.distributed.get_world_size()\n",
        "    print('my rank={} local_rank={}'.format(rank, local_rank))\n",
        "    torch.cuda.set_device(local_rank)\n",
        "    return {\n",
        "        'rank': rank,\n",
        "        'world_size': world_size,\n",
        "    }\n",
        "\n",
        "def set_up_env(env_params):\n",
        "    assert torch.cuda.is_available()\n",
        "    if env_params['distributed']:\n",
        "        env_params.update(\n",
        "            _torch_distributed_init_process_group(\n",
        "                local_rank=env_params['local_rank']))\n",
        "    env_params['device'] = torch.device('cuda')\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# OPTIMIZER AND SCHEDULER\n",
        "##############################################################################\n",
        "\n",
        "def _get_grad_requiring_params(model):\n",
        "    nb_parameters = 0\n",
        "    grad_requiring_params = []\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            nb_parameters += param.numel()\n",
        "            grad_requiring_params.append(param)\n",
        "    print('nb_parameters={:.2f}M'.format(nb_parameters / 1e6))\n",
        "    return grad_requiring_params\n",
        "\n",
        "\n",
        "def _get_optimizer(model,\n",
        "                   optim,\n",
        "                   lr: float,\n",
        "                   momentum: float,\n",
        "                   grad_clip: float):\n",
        "    if optim == 'sgd':\n",
        "        return torch.optim.SGD(_get_grad_requiring_params(model),\n",
        "                               lr=lr,\n",
        "                               momentum=momentum)\n",
        "    elif optim == 'adagrad':\n",
        "        return AdagradWithGradClip(_get_grad_requiring_params(model),\n",
        "                                   lr=lr,\n",
        "                                   grad_clip=grad_clip)\n",
        "    else:\n",
        "        raise RuntimeError(\"wrong type of optimizer \"\n",
        "                           \"- must be 'sgd' or 'adagrad\")\n",
        "\n",
        "\n",
        "def _get_scheduler(optimizer, lr_warmup):\n",
        "    if lr_warmup > 0:\n",
        "        return torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer, lambda ep: min(1, ep / lr_warmup))\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_optimizer_and_scheduler(model, optim_params):\n",
        "    optimizer = _get_optimizer(model=model,\n",
        "                               optim=optim_params['optim'],\n",
        "                               lr=optim_params['lr'],\n",
        "                               momentum=optim_params['momentum'],\n",
        "                               grad_clip=optim_params['grad_clip'])\n",
        "    scheduler = _get_scheduler(optimizer=optimizer,\n",
        "                               lr_warmup=optim_params['lr_warmup'])\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# CHECKPOINT\n",
        "##############################################################################\n",
        "\n",
        "def _load_checkpoint(checkpoint_path, model, optimizer, scheduler, logger,\n",
        "                     distributed):\n",
        "    print('loading from a checkpoint at {}'.format(checkpoint_path))\n",
        "    if distributed:\n",
        "        # the model is saved from gpu0 so we need to map it to CPU first\n",
        "        checkpoint_state = torch.load(\n",
        "            checkpoint_path, map_location=lambda storage, loc: storage)\n",
        "    else:\n",
        "        checkpoint_state = torch.load(checkpoint_path)\n",
        "    iter_init = checkpoint_state['iter_no'] + 1  # next iteration\n",
        "    model.load_state_dict(checkpoint_state['model'])\n",
        "    optimizer.load_state_dict(checkpoint_state['optimizer'])\n",
        "    logger.load_state_dict(checkpoint_state['logger'])\n",
        "    if 'scheduler_iter' in checkpoint_state:\n",
        "        # we only need the step count\n",
        "        scheduler.step(checkpoint_state['scheduler_iter'])\n",
        "    return iter_init\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, logger,\n",
        "                    distributed):\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        return _load_checkpoint(checkpoint_path=checkpoint_path,\n",
        "                                model=model,\n",
        "                                optimizer=optimizer,\n",
        "                                scheduler=scheduler,\n",
        "                                logger=logger,\n",
        "                                distributed=distributed)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def save_checkpoint(checkpoint_path, iter_no, model,\n",
        "                    optimizer, scheduler, logger):\n",
        "    if checkpoint_path:\n",
        "        checkpoint_state = {\n",
        "            'iter_no': iter_no,  # last completed iteration\n",
        "            'model': model.state_dict(),\n",
        "            'logger': logger.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        if scheduler is not None:\n",
        "            checkpoint_state['scheduler_iter'] = scheduler.last_epoch\n",
        "        torch.save(checkpoint_state, checkpoint_path)\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# LOGGER\n",
        "##############################################################################\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self._state_dict = dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._state_dict = state_dict\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self._state_dict\n",
        "\n",
        "    def _log(self, title, value):\n",
        "        if title not in self._state_dict:\n",
        "            self._state_dict[title] = []\n",
        "        self._state_dict[title].append(value)\n",
        "\n",
        "    def log_iter(self, iter_no, nb_batches_per_iter, loss_train, loss_val,\n",
        "                 elapsed, model):\n",
        "        step = (iter_no + 1) * nb_batches_per_iter\n",
        "        train_bpc = float(loss_train / math.log(2))\n",
        "        val_bpc = float(loss_val / math.log(2))\n",
        "        msg = 'steps: {}'.format(step)\n",
        "        msg += '\\ttrain: {:.3f}bpc\\tval: {:.3f}bpc'.format(train_bpc, val_bpc)\n",
        "        msg += '\\tms/batch: {:.1f}'.format(elapsed)\n",
        "        self._log(title='step', value=step)\n",
        "        self._log(title='train_bpc', value=train_bpc)\n",
        "        self._log(title='val_bpc', value=val_bpc)\n",
        "\n",
        "        if model.module.layers[0].attn.attn.adapt_span_enabled:\n",
        "            avg_spans = []\n",
        "            max_spans = []\n",
        "            for layer in model.module.layers:\n",
        "                avg_spans.append(\n",
        "                    layer.attn.attn.adaptive_span.get_current_avg_span())\n",
        "                max_spans.append(\n",
        "                    layer.attn.attn.adaptive_span.get_current_max_span())\n",
        "            span_avg = float(sum(avg_spans)) / len(avg_spans)\n",
        "            span_max = float(max(max_spans))\n",
        "            self._log('span_avg', span_avg)\n",
        "            self._log('span_max', span_max)\n",
        "            msg += \"\\tspan_avg: {:.0f}\\tspan_max: {:.0f}\".format(span_avg, span_max)\n",
        "\n",
        "        print(msg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e2xHJzjgu_J",
        "colab_type": "text"
      },
      "source": [
        "#### Монгол хэл дээрх Библийн датаг татах, боловсруулах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BINcX2dilLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(file_name):\n",
        "    if file_name == 'data/mnbible':\n",
        "        !mkdir -p data/mnbible\n",
        "        %cd data/mnbible\n",
        "        !echo \"Downloading mnbible data ...\"\n",
        "        !wget https://github.com/dl-ub-summer-school/2019/raw/master/Seminar5/mnbible8/mnbible.zip\n",
        "        !unzip mnbible.zip\n",
        "        !wget https://raw.githubusercontent.com/dl-ub-summer-school/2019/master/Seminar5/mnbible8/prep_mnbible.py\n",
        "        !python3 prep_mnbible.py\n",
        "        %cd ../..\n",
        "    else:\n",
        "        !echo \"data not found!\"\n",
        "        \n",
        "#get_data('data/mnbible')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gyx4UcDdv5n",
        "colab_type": "text"
      },
      "source": [
        "####  Сургалтын ерөнхий функц"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPo9bLSMdxGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def launch(env_params,\n",
        "           model_params,\n",
        "           adapt_span_params,\n",
        "           optim_params,\n",
        "           data_params,\n",
        "           trainer_params):\n",
        "    # ENVIRONMENT (device, distributed, etc.)\n",
        "    set_up_env(env_params)\n",
        "    device = env_params['device']\n",
        "    distributed = env_params['distributed']\n",
        "\n",
        "    if distributed == False or env_params['rank'] == 0:\n",
        "        print('model_params:\\t', model_params)\n",
        "        print('optim_params:\\t', optim_params)\n",
        "        print('data_params:\\t', data_params)\n",
        "        print('trainer_params:\\t', trainer_params)\n",
        "        print('adapt_span_params:\\t', adapt_span_params)\n",
        "\n",
        "    # DATA\n",
        "    train_data, val_data, test_data = get_train_val_test_data(\n",
        "        data_params=data_params,\n",
        "        env_params=env_params,\n",
        "        batch_size=trainer_params['batch_size'],\n",
        "        device=device)\n",
        "\n",
        "    # MODEL\n",
        "    model = TransformerSeq(\n",
        "        vocab_size=data_params['vocab_size'], **model_params,\n",
        "        adapt_span_params=adapt_span_params)\n",
        "    if distributed:\n",
        "        local_rank = env_params['local_rank']\n",
        "        model = model.to(device)\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[local_rank], output_device=local_rank)\n",
        "    else:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        model = model.to(device)\n",
        "\n",
        "    # OPTIMIZER AND SCHEDULER\n",
        "    optimizer, scheduler = get_optimizer_and_scheduler(\n",
        "        model=model, optim_params=optim_params)\n",
        "\n",
        "    # create logger\n",
        "    logger = Logger()\n",
        "\n",
        "    # resume training from last checkpoint if exists\n",
        "    iter_init = load_checkpoint(\n",
        "        trainer_params['checkpoint_path'], model, optimizer, scheduler,\n",
        "        logger, distributed)\n",
        "\n",
        "    if trainer_params['full_eval_mode']:\n",
        "        # evaluate the model on test data\n",
        "        with torch.no_grad():\n",
        "            loss_val = full_eval(model, optimizer, scheduler, val_data,\n",
        "                                 model_params['block_size'],\n",
        "                                 model_params['hidden_size'])\n",
        "            loss_test = full_eval(model, optimizer, scheduler, test_data,\n",
        "                                  model_params['block_size'],\n",
        "                                  model_params['hidden_size'])\n",
        "            if distributed:\n",
        "                # collect results into rank0\n",
        "                stats = torch.tensor(\n",
        "                    [loss_val, loss_test]).to(device)\n",
        "                torch.distributed.reduce(stats, 0)\n",
        "                if env_params['rank'] == 0:\n",
        "                    loss_val = stats[0] / env_params['world_size']\n",
        "                    loss_test = stats[1] / env_params['world_size']\n",
        "                else:\n",
        "                    return\n",
        "\n",
        "            print('val: {:.3f}bpc'.format(loss_val / math.log(2)))\n",
        "            print('test: {:.3f}bpc'.format(loss_test / math.log(2)))\n",
        "        return\n",
        "\n",
        "    # position of current batch\n",
        "    data_pos = [0] * 2\n",
        "    # initialize caches for train and valid\n",
        "    hid_cache = [[\n",
        "        torch.zeros(\n",
        "            train_data.size(0),\n",
        "            layer.attn.attn.get_cache_size(),\n",
        "            model_params['hidden_size']).to(device)\n",
        "        for layer in model.module.layers] for _ in range(2)]\n",
        "\n",
        "    nb_batches_per_iter = trainer_params['nb_batches_per_iter']\n",
        "    for iter_no in range(iter_init, trainer_params['nb_iter']):\n",
        "        t_sta = time.time()\n",
        "        loss_train, data_pos[0], hid_cache[0] = train_iteration(\n",
        "            model, optimizer, scheduler, train_data, nb_batches_per_iter,\n",
        "            model_params['block_size'], False, data_pos[0], hid_cache[0],\n",
        "            trainer_params['batch_split'])\n",
        "        elapsed = 1000 * (time.time() - t_sta) / nb_batches_per_iter\n",
        "        with torch.no_grad():\n",
        "            loss_val, data_pos[1], hid_cache[1] = train_iteration(\n",
        "                model, optimizer, scheduler, val_data, nb_batches_per_iter,\n",
        "                model_params['block_size'], True, data_pos[1], hid_cache[1],\n",
        "                trainer_params['batch_split'])\n",
        "\n",
        "        if distributed:\n",
        "            # collect results into rank0\n",
        "            stats = torch.tensor(\n",
        "                [loss_train, loss_val]).to(device)\n",
        "            torch.distributed.reduce(stats, 0)\n",
        "            if env_params['rank'] == 0:\n",
        "                loss_train = stats[0] / env_params['world_size']\n",
        "                loss_val = stats[1] / env_params['world_size']\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        logger.log_iter(iter_no, nb_batches_per_iter, loss_train,\n",
        "                        loss_val, elapsed, model)\n",
        "        save_checkpoint(trainer_params['checkpoint_path'],\n",
        "                        iter_no, model, optimizer, scheduler, logger)\n",
        "\n",
        "def main(args):\n",
        "    (env_params, model_params, adapt_span_params, optim_params, data_params, trainer_params) = get_params(args, PARAMS_CONFIG)\n",
        "    launch(env_params, model_params, adapt_span_params, optim_params, data_params, trainer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikLnGbXOmnsp",
        "colab_type": "text"
      },
      "source": [
        "#### Монгол хэл дээрх Библийн датан дээрх үсгэн түвшний хэлний моделийн сургалтын функц, үүнийгээ ашиглаж сургалт эхлүүлэх нь"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UieSKX8mpgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mn_lm():\n",
        "    # If run out of GPU memory, increase \"--batch-split\" argument.\n",
        "\n",
        "    # get the data\n",
        "    get_data('data/mnbible')\n",
        "    !mkdir -p checkpoints\n",
        "\n",
        "    args={\"data\": \"data/mnbible\",\n",
        "          \"nlayers\": 8,\n",
        "          \"hid-sz\": 256,\n",
        "          \"inner-hid-sz\": 1024,\n",
        "          \"nheads\": 1,\n",
        "          \"block-sz\": 256,\n",
        "          \"batch-sz\": 64,\n",
        "          \"lr\": 0.07,\n",
        "          \"momentum\": 0,\n",
        "          \"dropout\": 0,\n",
        "          \"optim\": \"adagrad\",\n",
        "          \"lr-warmup\": 8000,\n",
        "          \"grad-clip\": 0.03,\n",
        "          \"niter\": 1,\n",
        "          \"nbatches\": 1000,\n",
        "          \"checkpoint\": \"checkpoints/mnbible.pt\"}\n",
        "    \n",
        "    !echo \"Training ...\"\n",
        "    # using the pytorch distributed launching\n",
        "    main(args)\n",
        "\n",
        "\n",
        "    !echo \"Evaluation ...\"\n",
        "    # use a smaller batch size to reduce tokens without context and omitted tokens.\n",
        "    args[\"full-eval-mode\"] = True\n",
        "    args[\"batch-sz\"] = 8\n",
        "    main(args)\n",
        "\n",
        "mn_lm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg74EuurFP_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
